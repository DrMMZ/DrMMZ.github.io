{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN_numpy.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBn9ZeBcfSt4",
        "colab_type": "text"
      },
      "source": [
        "# **NumPy Implementation of Fully Connected Neural Network**\n",
        "\n",
        "*Ming Ming Zhang*\n",
        "\n",
        "Implement an $L$-layer ReLU network for any integer $L>0$ with $L_1$ or $L_2$-regularized softmax loss and stochastic gradient descent in Numpy.\n",
        "\n",
        "The implementation is based on *Derivatives in fully connected neural networks* by Ming Ming Zhang. In addition, numeric stability and gradients checking are added."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dab0OHh_fP_8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GcaBSqLxMTL",
        "colab_type": "text"
      },
      "source": [
        "**1. Implementation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8W1tn8ARgZ01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helper functions for fully connected layers.\n",
        "\n",
        "def linear_forward(H_prev, W, b):\n",
        "  \"\"\"\n",
        "  Computes the linear forward.\n",
        "\n",
        "  Inputs:\n",
        "  -H_prev: numpy of shape (D_in,N) where D_in is the dimension of H_prev and N is the number of examples\n",
        "  -W: numpy of shape (D_out,D_in) weight parameter, where D_out is the dimension of next layer\n",
        "  -b: numpy of shape (D_out,1) bias parameter\n",
        "\n",
        "  Outputs:\n",
        "  -Z: numpy of shape (D_out,N) linear forward output\n",
        "  -cache: tuple of (H_prev, W, b)\n",
        "  \"\"\"\n",
        "\n",
        "  Z = np.dot(W, H_prev) + b\n",
        "  cache = (H_prev, W, b)\n",
        "\n",
        "  return Z, cache\n",
        "\n",
        "\n",
        "def relu_forward(Z):\n",
        "  \"\"\"\n",
        "  Computes the ReLU activation forward.\n",
        "\n",
        "  Input:\n",
        "  -Z: numpy from linear forward\n",
        "\n",
        "  Outputs:\n",
        "  -H: numpy of shape (D_out,N)\n",
        "  -cache: Z \n",
        "  \"\"\"\n",
        "\n",
        "  H = np.maximum(0, Z)\n",
        "  cache = Z\n",
        "\n",
        "  return H, cache\n",
        "\n",
        "\n",
        "def relu_backward(dH, cache):\n",
        "  \"\"\"\n",
        "  Computes the ReLU activation backward.\n",
        "\n",
        "  Inputs:\n",
        "  -dH: numpy of shape (D_out,N) gradient of loss wrt H\n",
        "  -cache: Z from relu_forward\n",
        "\n",
        "  Ouput:\n",
        "  -dZ_prev: numpy of shape (D_in,N) gradient of loss wrt Z_prev\n",
        "  \"\"\"\n",
        "\n",
        "  Z_prev = cache\n",
        "\n",
        "  dReLU = np.zeros_like(Z_prev)\n",
        "  dReLU[Z_prev>0] = 1\n",
        "  dZ_prev = dH * dReLU\n",
        "\n",
        "  return dZ_prev\n",
        "\n",
        "\n",
        "def linear_backward(dZ, cache):\n",
        "  \"\"\"\n",
        "  Computes the linear backward.\n",
        "\n",
        "  Inputs:\n",
        "  -dZ: numpy of shape (D_out,N) gradient of loss wrt Z\n",
        "  -cache: tuple (H_prev, W, b) from linear_forward\n",
        "\n",
        "  Outputs:\n",
        "  -dH_prev: numpy of shape (D_in,N) gradient of loss wrt H_prev\n",
        "  -dW: numpy of shape (D_out,D_in) gradient of loss wrt W\n",
        "  -db: numpy of shape (D_out,1) gradient of loss wrt b\n",
        "  \"\"\"\n",
        "\n",
        "  H_prev, W, b = cache\n",
        "\n",
        "  dH_prev = np.dot(W.T, dZ)\n",
        "  dW = np.dot(dZ, H_prev.T)\n",
        "  db = np.sum(dZ, axis=1, keepdims=True)\n",
        "\n",
        "  return dH_prev, dW, db\n",
        "\n",
        "\n",
        "def softmax_loss(Z_L, y):\n",
        "  \"\"\"\n",
        "  Computes the softmax loss and its gradient.\n",
        "\n",
        "  Inputs:\n",
        "  -Z_L: numpy of shape (C,N) from linear_forward where C is number of classes\n",
        "  -y: numpy of shape (N,) true labels\n",
        "\n",
        "  Outputs:\n",
        "  -loss: scalar data loss\n",
        "  -dZ_L: numpy of shape (C,N) gradient of loss wrt Z_L\n",
        "  \"\"\"\n",
        "\n",
        "  N = Z_L.shape[1]\n",
        "  C = Z_L.shape[0] \n",
        "\n",
        "  # Numeric stability, by subtracting a value k max over scores Z_L\n",
        "  k = np.max(Z_L, axis=0).reshape((1,-1)) # (1, N)\n",
        "  Z_L -= k # (C, N)\n",
        "  H = np.exp(Z_L) / np.sum(np.exp(Z_L), axis=0, keepdims=True)\n",
        "  L = -np.log(H[y, range(N)]) # (C,N)\n",
        "  loss = np.sum(L) / N # a scalar\n",
        "\n",
        "  Y = np.zeros((C,N)) # one-hot vector of the label y\n",
        "  Y[y, range(N)] = 1\n",
        "  dZ_L = (H - Y) / N\n",
        "\n",
        "  return loss, dZ_L"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d6zRyIijpJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FCNet_NP(object):\n",
        "  \"\"\"\n",
        "  Build a fully connected L-layer neural network with ReLU activation function,\n",
        "  and L1/L2 regularized softmax loss function, i.e.,\n",
        "  {linear - relu} * (L-1) - linear - softmax.\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  def __init__(self, input_dim, hidden_dims, num_classes, reg=0.0, reg_method='L2', weight_scale=None, dtype=np.float32):\n",
        "    \"\"\"\n",
        "    Initialize a fully connected neural network.\n",
        "\n",
        "    Inputs:\n",
        "    -input_dim: integer>0 the size of input\n",
        "    -hidden_dims: list of integers>0 the size of l-th hidden layer for each l in {1,...,L-1}\n",
        "    -num_classes: integer>1 the number of classes to classify\n",
        "    -reg: scalar L1 or L2 regularization strength\n",
        "    -reg_method: string 'L1' or 'L2' regularization\n",
        "    -weight_scale: scalar the standard deviation for random initialization of the weights\n",
        "    -dtype: numpy datatype\n",
        "    \"\"\"\n",
        "\n",
        "    self.reg = reg\n",
        "    self.reg_method = reg_method\n",
        "    self.dtype = dtype\n",
        "    self.num_layers = 1 + len(hidden_dims) # number L of layers in the network\n",
        "    self.params = {}\n",
        "\n",
        "    layer_dims = [input_dim] + hidden_dims + [num_classes] # dimension of all l-layers for l in {0,1,...,L}\n",
        "    \n",
        "    for l in range(1, len(layer_dims)): # note that len(layer_dims)-1 is L the number of layers\n",
        "      if weight_scale is None:\n",
        "        self.params['W'+str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1]) * np.sqrt(2.0/layer_dims[l-1])\n",
        "      if weight_scale is not None:\n",
        "        self.params['W'+str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1]) * weight_scale\n",
        "      self.params['b'+str(l)] = np.zeros((layer_dims[l],1))\n",
        "\n",
        "    for k, v in self.params.items(): # cast parameters to the specified datatype\n",
        "      self.params[k] = v.astype(dtype)\n",
        "\n",
        "\n",
        "  def loss(self, X, y=None):\n",
        "    \"\"\"\n",
        "    Computes the training-time loss and gradients, and test-time classification scores.\n",
        "\n",
        "    Inputs:\n",
        "    -X: numpy of shape (D,N) data matrix, where D is the dimension and N is the number of examples\n",
        "    -y: numpy of shape (N,) true labels\n",
        "\n",
        "    Outputs:\n",
        "    -Z_L: numpy of shape (num_classes,N) scores (linear output of last layer) if y=None \n",
        "    -loss: scalar loss if y!=None\n",
        "    -grads: dictionary of gradients if y!=None\n",
        "    \"\"\"\n",
        "\n",
        "    X = X.astype(self.dtype)\n",
        "    L = self.num_layers\n",
        "    caches = []\n",
        "    H = X\n",
        "\n",
        "    for l in range(1,L):\n",
        "      H_prev = H\n",
        "      Z, cache_linear = linear_forward(H_prev, self.params['W'+str(l)], self.params['b'+str(l)])\n",
        "      H, cache_relu = relu_forward(Z)\n",
        "      cache = (cache_linear, cache_relu)\n",
        "      caches.append(cache)\n",
        "    \n",
        "    Z_L, cache = linear_forward(H, self.params['W'+str(L)], self.params['b'+str(L)])\n",
        "    caches.append(cache)\n",
        "\n",
        "    if y is None:\n",
        "      return Z_L\n",
        "\n",
        "    loss, dZ_L = softmax_loss(Z_L, y)\n",
        "\n",
        "    R = 0.0 # initialize regularization loss\n",
        "    for l in range(1,L+1):\n",
        "      if self.reg_method == 'L1': # L1-regularization loss\n",
        "        R += self.reg * np.sum(np.absolute(self.params['W'+str(l)]))\n",
        "      elif self.reg_method == 'L2': # L2-regularization loss\n",
        "        R += 0.5 * self.reg * np.sum(np.square(self.params['W'+str(l)]))\n",
        "    if self.reg_method == 'L1':\n",
        "      loss += R \n",
        "    elif self.reg_method == 'L2':\n",
        "      loss += R\n",
        "\n",
        "    grads = {}\n",
        "\n",
        "    dH_prev, dWL, dbL = linear_backward(dZ_L, cache)\n",
        "    dH_prev_temp = dH_prev\n",
        "    grads['W'+str(L)] = dWL\n",
        "    grads['b'+str(L)] = dbL\n",
        "    for l in reversed(range(L-1)):\n",
        "      dH = dH_prev_temp\n",
        "      cache_linear, cache_relu = caches[l]\n",
        "      dZ = relu_backward(dH, cache_relu)\n",
        "      dH_prev_temp, dW, db = linear_backward(dZ, cache_linear)\n",
        "      grads['W'+str(l+1)] = dW\n",
        "      grads['b'+str(l+1)] = db\n",
        "\n",
        "    for l in range(1,L+1):\n",
        "      if self.reg_method == 'L1':\n",
        "        grads['W'+str(l)] += self.reg * np.sign(self.params['W'+str(l)]) \n",
        "      elif self.reg_method == 'L2':\n",
        "        grads['W'+str(l)] += self.reg * self.params['W'+str(l)]\n",
        "\n",
        "    return loss, grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2se0oIo1QH8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Solver(object):\n",
        "  \"\"\"\n",
        "  Build a solver to train a model object which performs stochastic gradient descent update.\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  def __init__(self, model, data, **kwargs):\n",
        "    \"\"\"\n",
        "    Construct a solver.\n",
        "\n",
        "    Inputs:\n",
        "    -model: model object containing API model.params and model.loss described as above.\n",
        "    -data: dictionary of training and validation data, i.e., X_train, X_val, y_train, y_val\n",
        "\n",
        "    Optional arguments:\n",
        "    -lr: scalar step size (learning rate) in stochastic gradient descent update\n",
        "    -lr_decay: scalar for learning rate decay after each epoch\n",
        "    -batch_size: size of minibatches used to compute loss and gradient during training\n",
        "    -num_epochs: the number of epochs to run for during training\n",
        "    -print_every: integer>0 training losses will be printed every print_every iterations\n",
        "    -verbose: boolean, no output will be printed during training if set to false\n",
        "    -metric_method: string 'acc' or 'f1'\n",
        "    \"\"\"\n",
        "\n",
        "    self.model = model\n",
        "    self.X_train = data['X_train']\n",
        "    self.X_val = data['X_val']\n",
        "    self.y_train = data['y_train']\n",
        "    self.y_val = data['y_val']\n",
        "\n",
        "    self.lr = kwargs.pop('lr', 1e-2)\n",
        "    self.lr_decay = kwargs.pop('lr_decay', 1.0)\n",
        "    self.batch_size = kwargs.pop('batch-size', 32)\n",
        "    self.num_epochs = kwargs.pop('num_epochs', 10)\n",
        "    self.print_every = kwargs.pop('print_every', 10)\n",
        "    self.verbose = kwargs.pop('verbose', True)\n",
        "    self.metric_method = kwargs.pop('metric_method', 'acc')\n",
        "\n",
        "    self.__reset()\n",
        "\n",
        "  def __reset(self):\n",
        "    \"\"\"\n",
        "    Reset some varibles for optimization.\n",
        "    \"\"\"\n",
        "\n",
        "    self.epoch = 0\n",
        "    self.best_val = 0.0\n",
        "    self.best_params = {}\n",
        "    self.loss_history = []\n",
        "    self.train_history = []\n",
        "    self.val_history = []\n",
        "\n",
        "\n",
        "  def __step(self):\n",
        "    \"\"\"\n",
        "    A single step of gradient update used in train() below.\n",
        "    \"\"\"\n",
        "\n",
        "    num_train = self.X_train.shape[1]\n",
        "    batch_mask = np.random.choice(num_train, self.batch_size)\n",
        "    X_batch = self.X_train[:, batch_mask]\n",
        "    y_batch = self.y_train[batch_mask]\n",
        "\n",
        "    loss, grads = self.model.loss(X_batch, y_batch)\n",
        "    self.loss_history.append(loss)\n",
        "\n",
        "    for k, v in self.model.params.items():\n",
        "      dv = grads[k]\n",
        "      v += -self.lr * dv \n",
        "      self.model.params[k] = v\n",
        "\n",
        "\n",
        "  def check_prediction(self, X, y):\n",
        "    \"\"\"\n",
        "    Check prediction of the model on the given data.\n",
        "\n",
        "    Inputs:\n",
        "    -X: numpy of shape (D,N) data matrix\n",
        "    -y: numpy of shape (N,) labels\n",
        "    -metric_method: string 'acc' accuracy or 'f1' F1-score\n",
        "\n",
        "    Outputs:\n",
        "    -metric_score: scalar the score correctly classified by the model \n",
        "    \"\"\"\n",
        "\n",
        "    scores = self.model.loss(X)\n",
        "    y_hat = np.argmax(scores, axis=0)\n",
        "\n",
        "    if self.metric_method == 'acc':\n",
        "      metric_score = np.mean(y_hat == y)\n",
        "    elif self.metric_method == 'f1':\n",
        "      tp = np.sum((y_hat == 1) & (y == 1))\n",
        "      fp = np.sum((y_hat == 1) & (y == 0))\n",
        "      fn = np.sum((y_hat == 0) & (y == 1))\n",
        "      prec = tp / (tp+fp+1e-8) \n",
        "      rec = tp / (tp+fn+1e-8)\n",
        "      metric_score = 2*prec*rec / (prec+rec+1e-8)\n",
        "    \n",
        "    return metric_score\n",
        "\n",
        "\n",
        "  def train(self):\n",
        "    \"\"\"\n",
        "    Train the model using stochastic gradient descent.\n",
        "    \"\"\"\n",
        "\n",
        "    num_train = self.X_train.shape[1]\n",
        "    iterations_per_epoch = max(num_train // self.batch_size, 1)\n",
        "    num_iterations = self.num_epochs * iterations_per_epoch\n",
        "\n",
        "    for t in range(num_iterations):\n",
        "      self.__step()\n",
        "\n",
        "      if self.verbose and t % self.print_every == 0:\n",
        "        print('(Iteration %d / %d) loss: %f' % (t + 1, num_iterations, self.loss_history[-1]))\n",
        "\n",
        "      epoch_end = ((t + 1) % iterations_per_epoch == 0)\n",
        "      if epoch_end:\n",
        "        self.epoch += 1\n",
        "        self.lr *= self.lr_decay\n",
        "\n",
        "      first_it = (t == 0)\n",
        "      last_it = (t == num_iterations - 1)\n",
        "      if first_it or last_it or epoch_end:\n",
        "        train_score = self.check_prediction(self.X_train, self.y_train)\n",
        "        val_score = self.check_prediction(self.X_val, self.y_val)\n",
        "        self.train_history.append(train_score)\n",
        "        self.val_history.append(val_score)\n",
        "\n",
        "        if self.verbose:\n",
        "          print('(Epoch %d / %d) train_score: %f; val_score: %f' % (self.epoch, self.num_epochs, train_score, val_score))\n",
        "\n",
        "        if val_score > self.best_val:\n",
        "          self.best_val = val_score\n",
        "          self.best_params = {}\n",
        "          for k, v in self.model.params.items():\n",
        "            self.best_params[k] = v.copy()\n",
        "\n",
        "    self.model.params = self.best_params"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDoe3ZgYgY6R",
        "colab_type": "text"
      },
      "source": [
        "**2. Sanity Checks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tmpc1NZuoYSB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Gradient checking for 2-layer network\n",
        "\n",
        "def dictionary_to_vector(params):\n",
        "    \"\"\"\n",
        "    Convert parameters dictionary into a vector.\n",
        "    \n",
        "    Input:\n",
        "    -params: dictionary parameters W's and b's\n",
        "\n",
        "    Output:\n",
        "    -theta: the vector\n",
        "    -keys: list of keys of parameters\n",
        "    \"\"\"\n",
        "\n",
        "    keys = []\n",
        "    count = 0\n",
        "\n",
        "    for key in [\"W1\", \"b1\", \"W2\", \"b2\"]:\n",
        "        v = np.reshape(params[key], (-1,1))\n",
        "        keys = keys + [key] * v.shape[0]\n",
        "        \n",
        "        if count == 0:\n",
        "            theta = v\n",
        "        else:\n",
        "            theta = np.concatenate((theta, v), axis=0)\n",
        "        count = count + 1\n",
        "\n",
        "    return theta, keys\n",
        "\n",
        "\n",
        "def gradients_to_vector(grads):\n",
        "    \"\"\"\n",
        "    Convert gradients dictionary into a vector.\n",
        "\n",
        "    Input:\n",
        "    -grads: gradients dictionary\n",
        "\n",
        "    Ouput:\n",
        "    -theta: the vector\n",
        "    \"\"\"\n",
        "\n",
        "    count = 0\n",
        "\n",
        "    for key in [\"W1\", \"b1\", \"W2\", \"b2\"]:\n",
        "        v = np.reshape(grads[key], (-1,1))\n",
        "        \n",
        "        if count == 0:\n",
        "            theta = v\n",
        "        else:\n",
        "            theta = np.concatenate((theta, v), axis=0)\n",
        "        count = count + 1\n",
        "\n",
        "    return theta\n",
        "\n",
        "\n",
        "def vector_to_dictionary(theta):\n",
        "    \"\"\"\n",
        "    Convert a vector to parameters dictionary.\n",
        "\n",
        "    Input:\n",
        "    -theta: the vector\n",
        "\n",
        "    Ouput:\n",
        "    -params: parameters dictionary\n",
        "    \"\"\"\n",
        "\n",
        "    params = {}\n",
        "\n",
        "    params[\"W1\"] = theta[:12].reshape((3,4))\n",
        "    params[\"b1\"] = theta[12:15].reshape((3,1))\n",
        "    params[\"W2\"] = theta[15:21].reshape((2,3))\n",
        "    params[\"b2\"] = theta[21:24].reshape((2,1))\n",
        "\n",
        "    return params\n",
        "\n",
        "\n",
        "def gradient_check(model, X, y, epsilon=1e-7):\n",
        "    \"\"\"\n",
        "    Checks if the gradients of FCNet_NP computes correctly.\n",
        "    \n",
        "    Inputs:\n",
        "    -model: FCNet_NP\n",
        "    -X: random data matrix generated in gradient_check_data()\n",
        "    -y: true labels generated in gradient_check_data()\n",
        "    -epsilon: tiny shift in numerical gradient\n",
        "    \n",
        "    Output:\n",
        "    -difference: difference between the numerical and analytical gradient\n",
        "    \"\"\"\n",
        "  \n",
        "    params = model.params\n",
        "    _, grads = model.loss(X, y)\n",
        "    v_params, _ = dictionary_to_vector(params)\n",
        "    v_grads = gradients_to_vector(grads)\n",
        "    n = v_params.shape[0]\n",
        "    L_plus = np.zeros((n, 1))\n",
        "    L_minus = np.zeros((n, 1))\n",
        "    gradapprox = np.zeros((n, 1))\n",
        "    \n",
        "    for i in range(n):\n",
        "        thetaplus = np.copy(v_params)  \n",
        "        thetaplus[i][0] = thetaplus[i] + epsilon\n",
        "        paramsplus = vector_to_dictionary(thetaplus)\n",
        "        model.params = paramsplus\n",
        "        L_plus[i], _ = model.loss(X, y,)\n",
        "      \n",
        "        thetaminus = np.copy(v_params)  \n",
        "        thetaminus[i][0] = thetaminus[i] - epsilon\n",
        "        paramsminus = vector_to_dictionary(thetaminus)\n",
        "        model.params = paramsminus\n",
        "        L_minus[i], _ = model.loss(X, y)   \n",
        "       \n",
        "        gradapprox[i] = (L_plus[i] - L_minus[i]) / (2*epsilon)\n",
        "    \n",
        "    numerator = np.linalg.norm(v_grads - gradapprox) \n",
        "    denominator = np.linalg.norm(v_grads) + np.linalg.norm(gradapprox) \n",
        "    difference =  numerator / denominator \n",
        "\n",
        "    return difference"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CT9BVkGsin1P",
        "colab_type": "code",
        "outputId": "9e4acd24-16d0-4167-f4af-d739eec59b1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "# Gradient checking\n",
        "\n",
        "N, D, H1, C = 3, 4, 3, 2\n",
        "X = np.random.randn(D,N)\n",
        "y = np.array([1,1,0])\n",
        "\n",
        "reg_methods = ['L1', 'L2']\n",
        "for i in range(2):\n",
        "   model = FCNet_NP(input_dim=D, hidden_dims=[H1], num_classes=C, reg_method=reg_methods[i], dtype=np.float64)\n",
        "   for reg in [0.0, 0.8]:\n",
        "     model.reg = reg\n",
        "     error = gradient_check(model, X, y)\n",
        "     print('%s-regularization %.1f difference %.2e' % (reg_methods[i], reg, error))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "L1-regularization 0.0 difference 2.42e-09\n",
            "L1-regularization 0.8 difference 5.38e-09\n",
            "L2-regularization 0.0 difference 2.47e-09\n",
            "L2-regularization 0.8 difference 1.16e-09\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GX1elvYFsnf9",
        "colab_type": "code",
        "outputId": "cb7e38c1-1900-4903-b8dd-409ae4702f4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "# Loss checking\n",
        "\n",
        "N, D, H1, C = 3, 5, 50, 10\n",
        "X = np.random.randn(D,N)\n",
        "y = np.random.randint(C, size=(N,))\n",
        "\n",
        "correct_loss = -np.log(1/C)\n",
        "reg_methods = ['L1', 'L2']\n",
        "for i in range(2):\n",
        "   model = FCNet_NP(input_dim=D, hidden_dims=[H1], num_classes=C, reg_method=reg_methods[i], dtype=np.float64, weight_scale=1e-2)\n",
        "   for reg in [0.0, 1e1]:\n",
        "     model.reg = reg\n",
        "     loss, _ = model.loss(X, y)\n",
        "     print('%s-regularization %.1f initial loss %.2f, expected loss %.2f' % (reg_methods[i], reg, loss, correct_loss))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "L1-regularization 0.0 initial loss 2.30, expected loss 2.30\n",
            "L1-regularization 10.0 initial loss 58.86, expected loss 2.30\n",
            "L2-regularization 0.0 initial loss 2.30, expected loss 2.30\n",
            "L2-regularization 10.0 initial loss 2.66, expected loss 2.30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWjMJaOpzFmT",
        "colab_type": "code",
        "outputId": "ccf90943-e7f3-4458-e3a2-2f9cd97316da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 920
        }
      },
      "source": [
        "# Overfiting small random data\n",
        "\n",
        "np.random.seed(123)\n",
        "\n",
        "N, D, H1, C = 20, 50, 10, 4\n",
        "X = np.random.randn(D,N)\n",
        "y = np.random.randint(C, size=(N,))\n",
        "\n",
        "num_train = 15\n",
        "num_val = 5\n",
        "mask = list(range(num_train))\n",
        "X_train = X[:, mask]\n",
        "y_train = y[mask]\n",
        "mask = list(range(num_train, num_train+num_val))\n",
        "X_val = X[:, mask]\n",
        "y_val = y[mask]\n",
        "\n",
        "data = {}\n",
        "data['X_train'] = X_train\n",
        "data['X_val'] = X_val\n",
        "data['y_train'] = y_train\n",
        "data['y_val'] = y_val\n",
        "\n",
        "lr = 1\n",
        "metric_method = 'acc'\n",
        "\n",
        "model = FCNet_NP(input_dim=D, hidden_dims=[H1], num_classes=C)\n",
        "solver = Solver(model, data, print_every=10, num_epochs=10, lr=lr, metric_method=metric_method)\n",
        "solver.train()\n",
        "\n",
        "# Visualize training loss and train / val accuracy\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.title('Training loss')\n",
        "plt.plot(solver.loss_history, 'o')\n",
        "plt.xlabel('Iteration')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.title('Accuracy')\n",
        "plt.plot(solver.train_history, '-o', label='train')\n",
        "plt.plot(solver.val_history, '-o', label='val')\n",
        "plt.plot([0.5] * len(solver.val_history), 'k--')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='lower right')\n",
        "plt.gcf().set_size_inches(15, 12)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(Iteration 1 / 10) loss: 1.728099\n",
            "(Epoch 1 / 10) train_score: 0.533333; val_score: 0.200000\n",
            "(Epoch 2 / 10) train_score: 0.733333; val_score: 0.000000\n",
            "(Epoch 3 / 10) train_score: 0.933333; val_score: 0.000000\n",
            "(Epoch 4 / 10) train_score: 0.933333; val_score: 0.000000\n",
            "(Epoch 5 / 10) train_score: 1.000000; val_score: 0.000000\n",
            "(Epoch 6 / 10) train_score: 1.000000; val_score: 0.000000\n",
            "(Epoch 7 / 10) train_score: 1.000000; val_score: 0.000000\n",
            "(Epoch 8 / 10) train_score: 1.000000; val_score: 0.000000\n",
            "(Epoch 9 / 10) train_score: 1.000000; val_score: 0.000000\n",
            "(Epoch 10 / 10) train_score: 1.000000; val_score: 0.000000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x864 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}